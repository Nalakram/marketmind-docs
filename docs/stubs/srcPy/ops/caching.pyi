import time
from typing import Any as Incomplete
from dataclasses import dataclass, field
from enum import Enum
from pandas.util import hash_pandas_object as hash_pandas_object
from srcPy.utils.exceptions import DataError as DataError, InvalidInputError as InvalidInputError
from srcPy.utils.validators import validate_dataframe as validate_dataframe
from typing import Any, Callable, TypeVar

T = TypeVar('T')

class HashAlgorithm(Enum):
    """hash algorithm class."""
    XXHASH = 'xxhash'
    BLAKE3 = 'blake3'
    SIPHASH = 'siphash'
    SHA256 = 'sha256'

def hash_bytes(data: bytes, algo: HashAlgorithm = ...) -> str: ...
def hash_config(cfg_obj, algo: HashAlgorithm = ...) -> str: ...
def hash_dataframe_deterministic(df, cols=None, algo: HashAlgorithm = ...) -> str: ...
def versioned_key(*parts: str, version: str = 'v1') -> str: ...

class CompressionLevel(Enum):
    """compression level class."""
    NONE = 0
    FAST = 1
    HIGH = 2

@dataclass
class CompressionStrategy:
    """Strategy for compression behavior."""
    small_threshold: int = ...
    fast_threshold: int = ...
    def compress(self, data: bytes, level: CompressionLevel = None) -> tuple[bytes, CompressionLevel]: ...
    def decompress(self, data: bytes, level: CompressionLevel) -> bytes: ...

@dataclass
class CacheEntry:
    """cache entry class."""
    value: Any = ...
    expiry: float = ...
    version: int = ...
    access_count: int = ...
    last_access: float = field(default_factory=time.time)
    compression: CompressionLevel = ...

@dataclass
class CacheMetrics:
    """cache metrics class."""
    hits: int = ...
    misses: int = ...
    evictions: int = ...
    sets: int = ...
    total_latency_ns: int = ...
    @property
    def hit_rate(self) -> float: ...
    @property
    def avg_latency_us(self) -> float: ...

class AdaptiveTTLManager:
    """Manages adaptive ttl resources and operations."""
    base_ttl: Incomplete = ...
    volatility_multiplier: float = ...
    def __init__(self, base_ttl: float = 300) -> None: ...
    def get_ttl(self, key: Any, volatility: float = 0.0) -> float: ...
    def update_volatility(self, volatility: float): ...

class EnhancedCacheManager:
    """Manages enhanced cache resources and operations."""
    max_size: Incomplete = ...
    base_ttl: Incomplete = ...
    eviction_policy: Incomplete = ...
    enable_compression: Incomplete = ...
    compression: Incomplete = ...
    ttl_manager: Incomplete = ...
    metrics: Incomplete = ...
    def __init__(self, max_size: int = 128, ttl: float | None = None, eviction_policy: str = 'lru', enable_compression: bool = True, enable_metrics: bool = True) -> None: ...
    def get(self, key: Any, version: int = 0) -> Any | None: ...
    def set(self, key: Any, value: Any, ttl: float | None = None, version: int = 0, volatility: float = 0.0): ...
    def invalidate(self, key: Any): ...
    def invalidate_pattern(self, prefix: str): ...
    async def get_async(self, key: Any, version: int = 0) -> Any | None: ...
    async def set_async(self, key: Any, value: Any, **kwargs): ...

class DistributedCacheCoordinator:
    """distributed cache coordinator class."""
    CAS_SCRIPT: str = ...
    redis: Incomplete = ...
    def __init__(self, redis_client=None) -> None: ...
    async def cas_update(self, key: str, value: Any, timestamp: float) -> bool: ...
    async def invalidate_broadcast(self, channel: str, key: str) -> None: ...

def enhanced_cache(max_size: int = 128, ttl: float | None = None, key_fn: Callable | None = None, version: str = 'v1', enable_metrics: bool = True): ...

class PersistentCache:
    """persistent cache class."""
    cache_dir: Incomplete = ...
    compression: Incomplete = ...
    def __init__(self, cache_dir: str = '.cache', enable_compression: bool = True) -> None: ...
    def exists(self, key: str) -> bool: ...
    def save_df(self, key: str, df, version: str = 'v1'): ...
    def load_df(self, key: str, expected_version: str | None = None): ...
    def invalidate(self, key: str): ...
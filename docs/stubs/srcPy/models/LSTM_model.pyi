import torch
import torch.nn as nn
from _typeshed import Incomplete
from dataclasses import dataclass
from torch.utils.data import Sampler

log: Incomplete

@dataclass(kw_only=True)
class LSTMConfig:
    input_dim: int
    units: int = ...
    num_layers: int = ...
    zoneout_rate: float = ...
    input_dropout_rate: float = ...
    dropout: float = ...
    bidirectional: bool = ...
    return_sequences: bool = ...
    residual: bool = ...
    pooling_type: str | None = ...
    use_custom_cell: bool = ...
    seed: int | None = ...
    def to_dict(self): ...

class BucketBatchSampler(Sampler[list[int]]):
    batch_size: Incomplete
    buckets: list[list[int]]
    def __init__(self, dataset: list[tuple[torch.Tensor, torch.Tensor]], batch_size: int, boundaries: list[int]) -> None: ...
    def __iter__(self): ...
    def __len__(self) -> int: ...

def collate_fn(batch): ...

class SharedDropout(nn.Module):
    p: Incomplete
    def __init__(self, p: float) -> None: ...
    def train(self, mode: bool = True): ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class NormLSTMCell(nn.Module):
    W: Incomplete
    U: Incomplete
    b: Incomplete
    ln_i: Incomplete
    ln_f: Incomplete
    ln_g: Incomplete
    ln_o: Incomplete
    def __init__(self, inp: int, hid: int, zoneout: float) -> None: ...
    def forward(self, x: torch.Tensor, state: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]: ...

class NormLSTM(nn.Module):
    cell: Incomplete
    return_seq: Incomplete
    def __init__(self, inp: int, hid: int, zoneout: float, *, return_seq: bool) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None): ...

class BidirectionalNormLSTM(nn.Module):
    fwd: Incomplete
    bwd: Incomplete
    return_seq: Incomplete
    def __init__(self, inp: int, hid: int, zoneout: float, *, return_seq: bool) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None): ...

class LSTMBlock(nn.Module):
    cfg: Incomplete
    layers: Incomplete
    dropouts: Incomplete
    keep_seq_flags: Incomplete
    attention: Incomplete
    gate: Incomplete
    def __init__(self, cfg: LSTMConfig) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None): ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config): ...

class Model(nn.Module):
    cfg: Incomplete
    backbone: Incomplete
    head: Incomplete
    def __init__(self, cfg: LSTMConfig) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None) -> torch.Tensor: ...

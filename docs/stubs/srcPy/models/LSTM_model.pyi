import torch
import torch.nn as nn
from typing import Any as Incomplete
from dataclasses import dataclass
from srcPy.ops.mm_logkit import get_logger as get_logger
from srcPy.utils.exceptions import DataValidationError as DataValidationError, ModelInferenceError as ModelInferenceError, ModelTrainingError as ModelTrainingError
from srcPy.utils.torch_utils import seed_everything as seed_everything
from srcPy.utils.validators import validate_tensor as validate_tensor
from torch.utils.data import Sampler

log: Incomplete = ...

@dataclass(kw_only=True)
class LSTMConfig:
    """Configuration for lstm."""
    input_dim: int = ...
    units: int = ...
    num_layers: int = ...
    zoneout_rate: float = ...
    input_dropout_rate: float = ...
    dropout: float = ...
    bidirectional: bool = ...
    return_sequences: bool = ...
    residual: bool = ...
    pooling_type: str | None = ...
    use_custom_cell: bool = ...
    seed: int | None = ...
    def to_dict(self): ...

class BucketBatchSampler(Sampler[list[int]]):
    """bucket batch sampler class."""
    batch_size: Incomplete = ...
    buckets: list[list[int]] = ...
    def __init__(self, dataset: list[tuple[torch.Tensor, torch.Tensor]], batch_size: int, boundaries: list[int]) -> None: ...
    def __iter__(self): ...
    def __len__(self) -> int: ...

def collate_fn(batch): ...

class SharedDropout(nn.Module):
    """shared dropout class."""
    p: Incomplete = ...
    def __init__(self, p: float) -> None: ...
    def train(self, mode: bool = True): ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class NormLSTMCell(nn.Module):
    """norm lstm cell class."""
    W: Incomplete = ...
    U: Incomplete = ...
    b: Incomplete = ...
    ln_i: Incomplete = ...
    ln_f: Incomplete = ...
    ln_g: Incomplete = ...
    ln_o: Incomplete = ...
    def __init__(self, inp: int, hid: int, zoneout: float) -> None: ...
    def forward(self, x: torch.Tensor, state: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]: ...

class NormLSTM(nn.Module):
    """norm lstm class."""
    cell: Incomplete = ...
    return_seq: Incomplete = ...
    def __init__(self, inp: int, hid: int, zoneout: float, *, return_seq: bool) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None): ...

class BidirectionalNormLSTM(nn.Module):
    """bidirectional norm lstm class."""
    fwd: Incomplete = ...
    bwd: Incomplete = ...
    return_seq: Incomplete = ...
    def __init__(self, inp: int, hid: int, zoneout: float, *, return_seq: bool) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None): ...

class LSTMBlock(nn.Module):
    """lstm block class."""
    cfg: Incomplete = ...
    layers: Incomplete = ...
    dropouts: Incomplete = ...
    keep_seq_flags: Incomplete = ...
    attention: Incomplete = ...
    gate: Incomplete = ...
    def __init__(self, cfg: LSTMConfig) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None): ...
    def get_config(self): ...
    @classmethod
    def from_config(cls, config): ...

class Model(nn.Module):
    """model class."""
    cfg: Incomplete = ...
    backbone: Incomplete = ...
    head: Incomplete = ...
    def __init__(self, cfg: LSTMConfig) -> None: ...
    def forward(self, x: torch.Tensor, lengths: torch.Tensor | None = None) -> torch.Tensor: ...